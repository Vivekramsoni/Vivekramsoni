# -*- coding: utf-8 -*-
"""CreditCard_fraud 

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Piaj2SxPO8-RfgeYZ6U2LkCZz0h4QTHs

### (1) **Capstone Project 1** : **Credit_Card_Fraud_Detection **
## **Submitted By : VIVEK RAM SONI**
![credit card.jpg](data:image/jpeg;base64,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)

# Introduction
# Aim: Performing necessary feature engineering step to clean clean and modify the data. Identifying important features that may play important role to predict CREDIT CARD LABEL. Predicitng CREDIT CARD Fraud using Data analysis approach and machine learning model

**Q1. Why is your proposal important in today’s world? How predicting a good client is worthy for a bank? **

Ans. My proposal is important in today’s world because as I learn data science and machine learning I am not giving any proposal to any client just by looking current scenario or by any third person suggestion. I am coming to proposal point by analyzing the previous data of client and by creating a statically test train data with the help of machine learning algorithms finally I am reaching to the outcome which will be complete solution of client problem scientifically.
Prediction a good client is worthy for a bank because it has been seen that fraud case arising day by day and it leads to the down the capital as well as reputation of the bank. Bank is also always look for a customer which is good in term of not defaulter for the same bank ask client to proof his income , Address ,family-members and so many things but at last when the client appears as a fraud the bank fails. But by analyzing the same data we data science engineer can predict a accurate result which saves the bank from fraud.

**Q2. How is it going to impact the banking sector? **
Ans. The credit card fraud on bank is not a case of any particular bank. We can see this in all over the world’s bank and by analyzing this fraud case a report has come which says Year after year, credit card fraud attempts are on the rise, with a staggering 46% year-over-year increase reported globally. As credit card transactions continue to rise, the number of attempted fraud transactions follows suit.
Here are some ways in which credit card fraud can impact the banking sector:
1) Financial Losses for Banks: Banks and credit card issuers are usually responsible for reimbursing customers for fraudulent charges. This can result in substantial financial losses for banks, affecting their profitability and potentially leading to higher costs for consumers in terms of fees and interest rates.
2) Reputation Damage: Credit card fraud can erode customer trust in banks and financial institutions. If customers experience fraudulent charges and the bank's response is slow or inadequate, it can lead to negative publicity and a damaged reputation. Customers may start seeking alternatives, impacting customer retention.
3) Increased Operational Costs: Banks need to invest in fraud detection and prevention measures, which include implementing advanced security technologies, monitoring systems, and hiring cybersecurity experts. These investments can lead to increased operational costs for banks.
4) Resource Diversion: Banks have to allocate resources to investigate and address credit card fraud incidents. This diversion of resources can take away focus from other strategic initiatives that could benefit customers and the institution itself.
5) Innovation Hurdles: Fear of fraud can sometimes deter customers from adopting new payment technologies and digital banking solutions. This can slow down the adoption of innovative products and services in the banking sector.
6) Impact on Loan Approval: Credit card fraud can impact an individual's credit score, which can, in turn, affect their ability to secure loans or favorable interest rates in the future.etc.
To mitigate these impacts, banks need to continuously invest in robust security measures, stay updated on emerging fraud trends, and work closely with regulatory bodies to ensure compliance. Customer education about safe online practices and the steps they can take to protect their credit card information is also crucial in preventing credit card fraud and minimizing its impact on both customers and the banking sector.

**Q3. If any, what is the gap in the knowledge or how your proposed method can be helpful if required in future for any bank in India.**
 Ans. Data science and machine learning can play a crucial role in helping banks in India (and globally) address the challenges posed by credit card fraud and minimize its impact. Here's how these technologies can be utilized to fill the gaps and enhance fraud prevention efforts:
1) Advanced Fraud Detection: Machine learning algorithms can analyze vast amounts of transaction data in real-time to identify patterns and anomalies associated with fraudulent activities. By continuously learning from new data, these algorithms can adapt to evolving fraud techniques and detect suspicious transactions more accurately than traditional rule-based systems.
2) Behavioral Analysis: Data science techniques can be employed to create customer profiles and analyze their spending behaviors. This enables the identification of deviations from normal patterns, helping to flag potentially fraudulent activities even when the transactions seem legitimate.
3) Predictive Analytics: By analyzing historical data, machine learning models can predict potential fraud risks and trends. This proactive approach allows banks to take preventive measures before fraudulent activities escalate.
4) Multi-Layered Security: Machine learning can be used to implement multi-layered security systems. For example, biometric authentication (fingerprint, facial recognition) combined with transaction history analysis can provide a stronger defense against unauthorized access and transactions.
5) Customer Authentication: Advanced authentication methods, such as adaptive authentication, can use data science techniques to assess risk factors and apply the appropriate level of authentication for each transaction or interaction. This reduces friction for legitimate customers while deterring fraudsters.
6) Real-time Alerts: Machine learning algorithms can generate real-time alerts for suspicious activities. This enables banks to take immediate action, such as contacting the customer or blocking a potentially fraudulent transaction.
7) Network Analysis: Data science can help identify relationships and connections between seemingly unrelated entities involved in fraud. This can uncover larger fraud networks and assist law enforcement agencies in taking down organized fraud rings.
8) Continuous Learning: Machine learning models can continually learn from new data, adapting to changes in fraud tactics. This adaptability is crucial as fraudsters often modify their strategies to avoid detection.
9) Fraud Prevention Strategies: By analyzing historical fraud cases and their characteristics, data science can help banks develop more effective fraud prevention strategies, ensuring that resources are allocated where they are most needed.
10) Regulatory Compliance: Data science can assist banks in meeting regulatory requirements by providing detailed insights into their fraud prevention efforts and demonstrating due diligence in protecting customer data.
While data science and machine learning hold great promise, successful implementation requires skilled data scientists, access to high-quality data, and a commitment to ongoing refinement of models and strategies. Additionally, collaboration among banks, industry bodies, and regulatory authorities is crucial for sharing insights and best practices in combating credit card fraud effectively.

# Start With DA Hypothesis

1st i have to import some libraries to my environment
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

"""**For upload raw data file in colab i have to put these code**"""

from google.colab import files
uploaded = files.upload()

"""**It seems that i have two different files for analysis so i am assigning separate name like data1 and data2**."""

data_1 = pd.read_csv('Credit_card.csv')

data_2 = pd.read_csv('Credit_card_label.csv')

"""**To batter understanding and analsis i have to merge both the files because in one file Data_1 all variables or independents are there and in another file dependent variable is there.**"""

data_credit = pd.merge(data_1,data_2 ,on = 'Ind_ID' )

"""**Now to see all my data structure i have to use head function as:-**"""

data_credit.head()

"""**Here in Label column i found that 1 shows Fraud Customer and 0 shows Good Customer.**

**To avoid my Data set permanent deformation i am making one copy of my data set as:-**
"""

data = data_credit.copy()

"""**Exploration od Data**"""

data.info()

"""**With this Information i have seen all column haveing no.of rows count ,Missing values and data types.**"""

data.shape

data.columns

"""**By describe function i can get all relevent information which i need for the analysis.**"""

data.describe(include = 'all')

"""**Now i am going to check all the data column separatly and try to find any pattern by graph as:-**"""

# plot the histogram of each parameter
data.hist(figsize = (20, 20))
plt.show()

"""1) What approach are you going to take in order to prove or disprove your hypothesis?

Ans.

What feature engineering techniques will be relevant to your project?

Please justify your data analysis approach.

Identify important patterns in your data using the EDA approach to justify your findings.

**To look after the complete data set .I found that so many missing values are there now it time to deal with the same but before i have to convert all object data into the numeric data for batter analysis**

**Dealing with conversion of  Object to Numeric Data**
"""

data.isnull().sum()

"""**In Gender column 7 missing values ,Annual_income 23 missing values,Birthday_count 22 missing values,Type_Occupation having 488 missing value (very High).Lets go with this.**"""

# checking if observation has missing values more than 1
Missing_I = data.isnull().sum(axis = 1)
Missing_I = Missing_I[Missing_I > 1]
Missing_I

# exploring all rows and columns of the data set
pd.options.display.max_columns = None
pd.options.display.max_rows = None
display(data)

"""**Making a type of  matix to check and quick analysis of missing value**"""

# visualization of missing values
import missingno as msno
msno.matrix(data)

"""**Apart from type_ocupetion column, rest other columns have very less missing values.**
**Either i have to remove them or replace with something like mean or median or any other method.**
**Label (Result)  data has no missing values.**
**Here i found that birthday count is shows in no. of days and also total employment duration is also in same pattern.I have to convert it in no. of years for better understanding.**
"""

data["Birthday_count"] = (data["Birthday_count"] / 360)

data["Employed_days"] = (data["Employed_days"] / 360)

data.head()

"""**Conversion is sussefull but still both the converted column showing its all value in negative to deal with this again i have to make it positive as:-**"""

data["Birthday_count"] = -1 * data["Birthday_count"]

data["Employed_days"] = -1 * data["Employed_days"]

"""**For my refrence only i am converting the column head as mention Birthday count = Age and Employed_days = Work Exp:**"""

data.rename(columns={"Birthday_count": "Age"}, inplace=True)

data.rename(columns={"Employed_days": "Work_exp"}, inplace=True)

"""**Before dealing with missing values we have to encode all categorical data into numeric data.**

**Let me check how many unique category are there in each non numeric column one by one and what type.**
"""

data['GENDER'].unique()

data['Car_Owner'].unique()

data['Propert_Owner'].unique()

data['Type_Income'].unique()

data['EDUCATION'].unique()

data['Marital_status'].unique()

data['Housing_type'].unique()

data['Type_Occupation'].unique() # found that total 18 variables are there

"""**Encoding starts from here i have use here Ordinal Encoding to keep the length of my column 19 only**"""

data['Car_Owner'] = data['Car_Owner'].map({'Y':0, 'N':1})

data['GENDER'] = data['GENDER'].map({'M':1, 'F':0})

data['Propert_Owner'] = data['Propert_Owner'].map({'Y':0, 'N':1})

"""**Priority i have taken as 'Pensioner':3, 'Commercial associate':1, 'Working':2, 'State servant':0**"""

data['Type_Income'] = data['Type_Income'].map({'Pensioner':3, 'Commercial associate':1, 'Working':2, 'State servant':0})

"""**'Married':0, 'Single / not married':1, 'Civil marriage':2, 'Separated':3 'Widow':4**"""

data['Marital_status'] = data['Marital_status'].map({'Married':0, 'Single / not married':1, 'Civil marriage':2, 'Separated':3,
       'Widow':4})

data['Housing_type'] = data['Housing_type'].map({'House / apartment':1, 'With parents':2, 'Rented apartment':3,
       'Municipal apartment':4, 'Co-op apartment':4, 'Office apartment':0})

data['EDUCATION'] = data['EDUCATION'].map({'Higher education':1, 'Secondary / secondary special':2,
       'Lower secondary':3, 'Incomplete higher':3, 'Academic degree':0})

"""**Now it is big challange to deal here there are 18 variables in Type of occupation to deal withe same i am taking common variables according to rank in office/ industry like 'Core staff' mapped to 0,'Managers':0 ,'High skill tech staff':0,IT staff':0,'Secretaries':0,'Sales staff':1, 'Private service staff':1,'Security,HR staff':1 staff':1,'Accountants':1,'Medicine staff':1,'Cooking staff':2, 'Laborers':2,'Cleaning staff':2,'Drivers':2,'Low-skill Laborers':2,'Waiters/barmen staff':2.**"""

data['Type_Occupation'] = data['Type_Occupation'].map({'Core staff':0, 'Cooking staff':2, 'Laborers':2, 'Sales staff':1,
       'Accountants':1, 'High skill tech staff':0, 'Managers':0,
       'Cleaning staff':2, 'Drivers':2, 'Low-skill Laborers':2, 'IT staff':0,
       'Waiters/barmen staff':2, 'Security staff':1, 'Medicine staff':1,
       'Private service staff':1, 'HR staff':1, 'Secretaries':0})

"""**To avoid data leakage from outcom variable splitting the data set into two parts:-** **Independent and Dependent**"""

Independent = data.drop('label',axis=1)

Independent.head()

Dependent = data['label'] # creating Y variable only
Dependent.head()

"""**Now after conversion from Object data to numeric data in all rows time to go for missing value check.**"""

msno.bar(data)

"""In column Type_Occupation Total 490 Datas are missing which is very high about 31.6%"""

msno.heatmap(data) # Heat Map for Missing value correlation

"""# **Dealing with missing values**"""

GENDER1 = Independent['GENDER'].mode()[0]

Independent['GENDER'].fillna(GENDER1 , inplace=True)

Independent['Age'].mean()

Age1 = Independent['Age'].mean()

Independent['Age'].fillna(Age1 , inplace=True)

Independent["Annual_income"] = Independent["Annual_income"]/100000

Independent.rename(columns={"Annual_income_in_lac": "Annual_income"}, inplace=True)

Annual_income1 = Independent['Annual_income'].mean()

Independent['Annual_income'].fillna(Annual_income1 , inplace=True)

Independent['Type_Occupation'] = Independent['Type_Occupation'].fillna(3.0)

Independent.describe()

Independent.head()

"""**Now i will check the missing value fillup with another mthod too**"""

# Imputation using KNN
from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder

# Create a KNN imputer
knn_imputer = KNNImputer(n_neighbors=20)

Independent_knn = data.drop('label',axis=1).copy(deep=True)

# Encode categorical columns
label_encoder = LabelEncoder()
Independent_knn["GENDER"] = label_encoder.fit_transform(Independent_knn["GENDER"])
Independent_knn["Annual_income"] = label_encoder.fit_transform(Independent_knn["Annual_income"])
Independent_knn["Type_Occupation"] = label_encoder.fit_transform(Independent_knn["Type_Occupation"])

# Apply KNN imputation to the entire DataFrame (assuming all columns are numeric)
Independent_knn.iloc[:, :] = knn_imputer.fit_transform(Independent_knn)

"""**Use MICE imputation**"""

from sklearn.experimental import enable_iterative_imputer

from sklearn.impute import IterativeImputer

iter_imputer = IterativeImputer()

Independent_MICE = data.drop('label',axis=1).copy(deep=True)

Independent_MICE["GENDER"] = label_encoder.fit_transform(Independent_MICE["GENDER"])
Independent_MICE["Annual_income"] = label_encoder.fit_transform(Independent_MICE["Annual_income"])
Independent_MICE["Type_Occupation"] = label_encoder.fit_transform(Independent_MICE["Type_Occupation"])
Independent_MICE["Age"] = label_encoder.fit_transform(Independent_MICE["Age"])

Independent_MICE.iloc[:, :] = iter_imputer.fit_transform(Independent_MICE)

"""**Now check best one on these three imputations**"""

fig, ax =plt.subplots(1,3)
sns.histplot(Independent['GENDER'],bins=20, color="purple", ax=ax[0])
sns.histplot(Independent_knn['GENDER'], color="red", bins=20, ax=ax[1])
sns.histplot(Independent_MICE['GENDER'], color="green", bins=20, ax=ax[2])

fig, ax =plt.subplots(1,3)
sns.histplot(Independent['Annual_income'],bins=20, color="purple", ax=ax[0])
sns.histplot(Independent_knn['Annual_income'], color="red", bins=20, ax=ax[1])
sns.histplot(Independent_MICE['Annual_income'], color="green", bins=20, ax=ax[2])

fig, ax =plt.subplots(1,3)
sns.histplot(Independent['Type_Occupation'],bins=20, color="purple", ax=ax[0])
sns.histplot(Independent_knn['Type_Occupation'], color="red", bins=20, ax=ax[1])
sns.histplot(Independent_MICE['Type_Occupation'], color="green", bins=20, ax=ax[2])

fig, ax =plt.subplots(1,3)
sns.boxplot(Independent['Type_Occupation'], color="purple", ax=ax[0])
sns.boxplot(Independent_knn['Type_Occupation'], color="red", ax=ax[1])
sns.boxplot(Independent_MICE['Type_Occupation'], color="green", ax=ax[2])

Independent.describe()

Independent_knn.describe() # using describe function to see if there is any discrepency in numerical measures

Independent_MICE.describe() # using describe function to see if there is any discrepency in numerical measures

"""**From overall exploration it seems that Manual imputation by mean ,mode, MICE and KNN all are performed well and KNN imputation is performing best among all Hence, I will go ahead with Manual imputation by mean ,mode.**"""

Independent.info()

"""**Now the all the Missing values are filled with relevent values and All the data types are converted into the numeric type**"""

data_miss = pd.concat([Independent, Dependent], axis=1) # concatenating independent and dependent variable
data_miss.head(20)

data_miss.shape

"""**DATA SCALING: After above imputation i found that the column Ind_ID,Annual_income and Age are not matching to data set scale provided so there is a need to rescale the above column data.**

**For this i need to make it standard scale**
"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

data_miss['Annual_income'] = sc.fit_transform(pd.DataFrame(data_miss['Annual_income']))

data_miss['Age'] = sc.fit_transform(pd.DataFrame(data_miss['Age']))

data_miss['Work_exp'] = sc.fit_transform(pd.DataFrame(data_miss['Work_exp']))

data_miss.head(10)

"""**I am dropping the Ind_IDs as it is not required for me and it is not affecting my result.**"""

data_miss = data_miss.drop('Ind_ID',axis =1)

data_miss.describe()

"""**Dealing with outliers**

**From describe function it seems that in column Children,annual income,work exp and family members there are outliers present that have to remove**

**Here I am using IQR to remove outliers. I have already checked Z score and did not work well**
"""

Q1 = data_miss['Work_exp'].quantile(0.25)
Q3 = data_miss['Work_exp'].quantile(0.75)
print(Q3, Q1)
IQR = Q3 - Q1
print(IQR)
upper_bound = Q3 + 1.5 * IQR
lower_bound = Q1 - 1.5 * IQR
print(upper_bound)
print(lower_bound)

data_miss_out = data_miss[data_miss.Work_exp < upper_bound]
data_miss_out = data_miss_out[data_miss_out.Work_exp > lower_bound]
data_miss_out.shape

sns.histplot(data_miss_out['Work_exp'])

data_miss_out.describe()

"""**Now i am checking Annual_income outlier it seems 15.75 Lac per annum salary is possible but let me check the differences in limit.**"""

sns.histplot(data_miss_out['Annual_income'])

"""**YEs it has to be treat as outlier no i am going for the same**"""

Q1 = data_miss['Annual_income'].quantile(0.25)
Q3 = data_miss['Annual_income'].quantile(0.75)
print(Q3, Q1)
IQR = Q3 - Q1
print(IQR)
upper_bound = Q3 + 1.5 * IQR
lower_bound = Q1 - 1.5 * IQR
print(upper_bound)
print(lower_bound)

data_miss_out1 = data_miss_out[data_miss_out.Annual_income < upper_bound]
data_miss_out1 = data_miss_out1[data_miss_out1.Annual_income > lower_bound]
data_miss_out1.shape

data_miss_out1.describe()

"""**NOw checking for Family members**"""

sns.histplot(data_miss_out['Family_Members'])

"""**Outlier seen**"""

Q1 = data_miss['Family_Members'].quantile(0.25)
Q3 = data_miss['Family_Members'].quantile(0.75)
print(Q3, Q1)
IQR = Q3 - Q1
print(IQR)
upper_bound = Q3 + 1.5 * IQR
lower_bound = Q1 - 1.5 * IQR
print(upper_bound)
print(lower_bound)

data_miss_out2 = data_miss_out1[data_miss_out1.Family_Members < upper_bound]
data_miss_out2 = data_miss_out2[data_miss_out2.Family_Members > lower_bound]
data_miss_out2.shape

data_miss_out2.describe()

"""**Even after removing outliers columns are still skewed. For linear regression, errors should be normally distributed However, as per central limit theorem if sample size is more than 30 we are good to go.**

**# checking correlation**
"""

data_n = pd.DataFrame(data_miss_out2) # checking correlation
print(data_n.corr(method = 'spearman'))

# Examine multicollinearity using VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

# the independent variables set
X_vif = data_miss_out2.drop(['label'], axis=1)

# VIF dataframe
vif_data = pd.DataFrame()
vif_data["feature"] = X_vif.columns

# calculating VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i)
                          for i in range(len(X_vif.columns))]

print(vif_data)

"""**it seems that Family members,Marital status,children are highly corellated**"""

X_vif = data_miss_out2.drop(['label', 'CHILDREN', 'Marital_status','Mobile_phone','Family_Members'], axis=1)

# VIF dataframe
vif_data = pd.DataFrame()
vif_data["feature"] = X_vif.columns

# calculating VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i)
                          for i in range(len(X_vif.columns))]

print(vif_data)

"""**After formation on cleaned data set i have compare each variable/column to my** **result how they are it related and what kind of pattern variables are making to** **form result.And i found that:**

1) in occupation column ony IT staff are doing 100% Fraud on credit card and Waiters and low grade workers are doing 80% fraud on credit card.
2) In marriage column max fraud are coming from Widows about 26.31%
3) Housing type the people who are living in office appartment doing 28.57% fraud and living on muncipal houses are doing 36.40% fraud.
4) in gender column no any good result coming from DA approach but Men are higher in fraud as compair to women.
5)In Family member column when the family count come more than 5 ther are 100% fraud and also when family count is 1 they are also 13% fraud.
6)Education level when it falls lower education 42% fraud coming for credit card.
7) Children column more than 3 child doing highest fraud in data base.
8)car owner not giving any result for fraud case.
9)Salary if salary is in range 36000 to 63000 per annum they dont have any fraud record. and also if salary is more than 700000 per annum they dont have fraud records.
10) Age

# **MACHINE LEARNING APPROACH**

**Feature selection**
"""

data_miss_out2.describe()

sns.displot(np.sqrt(data_miss_out2['Work_exp']), kde = True) # np.sqrt(data_miss_out2['Work_exp'])

sns.displot(np.sqrt(data_miss_out2['Annual_income']), kde = True)

sns.displot(np.sqrt(data_miss_out2['Age']), kde = True)

sns.displot(np.sqrt(data_miss_out2['Family_Members']), kde = True)

sns.displot(np.sqrt(data_miss_out2['Housing_type']), kde = True)

"""**Now checking for any Duplicate value in data set.**"""

data_miss_out2.duplicated().any()

data_miss_out2 = data_miss_out2.drop_duplicates()

data_miss_out2.shape

"""**From sqrt and Box Cox transformation i have checked and  select some feature which are highly corelated with Result**

# **Now handling Imbalance DATA**
"""

data_miss_out2['label'].value_counts()

"""**by this value count i have seen total good customers are 921 and fraud customers are 89 which are highly imbalance in data set so tere is need to balance the data.**"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=data_miss_out2, x='label')
plt.show()

data_miss_out2.head()

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

X = data_miss_out2.drop(['label','Car_Owner','Mobile_phone','Work_Phone','Phone','EMAIL_ID'], axis=1) #'Work_exp','Annual_income','Age','Family_Members','Housing_type' are highly correlated
y = data_miss_out2['label']

X.shape

#  Split the Train and Test Data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""**Handling imbalance data set**
**1) Under sampling ** **2) Over sampling **

**Start with Logistic regression as i know my out put column is having binary values either 0 or 1.**
"""

logisticRegr = LogisticRegression()

logisticRegr.fit(X_train, y_train)

y_pred = logisticRegr.predict(X_test)
accuracy= logisticRegr.score(X_test,y_test)
print(y_pred)
print(accuracy)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

conf_mat = metrics.confusion_matrix(y_test, y_pred)
print(conf_mat)
confusion_matrix(y_test, logisticRegr.predict(X_test))
plt.show()

print("The accuracy of the model = ", round(accuracy*100,2), "%")

"""**Here i got 92.4% accuracy lets check in some other method.**"""

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

# by precision_score and recall_score and f1_score also i am checking

from sklearn.metrics import precision_score , recall_score , f1_score

precision_score(y_test, y_pred, zero_division=1)

recall_score(y_test, y_pred)

f1_score(y_test, y_pred, zero_division=1)

"""**Decision Tree classifier**"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)

y_pred2 = dt.predict(X_test)

accuracy_score(y_test,y_pred2)

precision_score(y_test, y_pred2)

recall_score(y_test, y_pred2)

f1_score(y_test, y_pred2)

"""**Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train,y_train)

y_pred3 = rf.predict(X_test)

accuracy_score(y_test,y_pred3)

precision_score(y_test, y_pred3)

recall_score(y_test, y_pred3)

f1_score(y_test, y_pred3)

final_data_us = pd.DataFrame({'Models': ['LR','DT','RF'],'ACC':[accuracy_score(y_test,y_pred)*100,
                                                                accuracy_score(y_test,y_pred2)*100,
                                                                accuracy_score(y_test,y_pred3)*100]})

final_data_us

# sns.barplot(final_data_us['Models'],final_data_us['ACC'])   this code showing errorso

sns.barplot(data=final_data_us, x='Models', y='ACC')

"""**This was with under samplig techniq no going for OVER SAMPLING**

**Over sampling**
"""

X = data_miss_out2.drop(['label'], axis=1)
y = data_miss_out2['label']

X.shape

y.shape

from imblearn.over_sampling import SMOTE

X_res,y_res = SMOTE().fit_resample(X,y)

y_res.value_counts()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.25, random_state=42)

logisticRegr = LogisticRegression()

logisticRegr.fit(X_train,y_train)

y_pred11 = logisticRegr.predict(X_test)

accuracy_score(y_test,y_pred11)

precision_score(y_test, y_pred11)

recall_score(y_test, y_pred11)

"""**Decision tree classifier**"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)

y_pred22 = dt.predict(X_test)

accuracy_score(y_test,y_pred22)

precision_score(y_test, y_pred22)

recall_score(y_test, y_pred22)

f1_score(y_test, y_pred22)

"""**Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train,y_train)

rf.fit(X_train,y_train)

y_pred33 = rf.predict(X_test)

accuracy_score(y_test,y_pred33)

precision_score(y_test, y_pred33)

recall_score(y_test, y_pred33)

f1_score(y_test, y_pred33)

final_data_os = pd.DataFrame({'Models': ['LR','DT','RF'],'ACC':[accuracy_score(y_test,y_pred11)*100,
                                                                accuracy_score(y_test,y_pred22)*100,
                                                                accuracy_score(y_test,y_pred33)*100]})

final_data_os

sns.barplot(data=final_data_us, x='Models', y='ACC')

"""**SO in machine learning I have seen that my unbalanced data set is treated by over sampling technique and after that random forest classifier accuracy came 93.27 %.**

**Now i am making a final model for prediction**
"""

rf1 = RandomForestClassifier()

rf1.fit(X_res,y_res)

import joblib

joblib.dump(rf1,'credit_card')

"""**In future by this model 'credit_card' we can perform predictions as:-**"""

joblib.load('credit_card')

"""Now suppose one customer name Chirag come to bank for credit card and give his information as :- 'GENDER'= M/1, 'Car_Owner' = YES/0, 'Propert_Owner' = Y/0, 'CHILDREN' = 3,
       'Annual_income' =4.5 , 'Type_Income' = 1, 'EDUCATION'= 0, 'Marital_status' = 1,
       'Housing_type' = 0, 'Birthday_count' = 32 , 'Employed_days' = 10, 'Mobile_phone' = 0,
       'Work_Phone' =1 , 'Phone'= 1, 'EMAIL_ID' = 0, 'Type_Occupation' = 0, 'Family_Members' = 3
"""

CHIRAG = joblib.load('credit_card')

PREDICTION = CHIRAG.predict([[1,0,0,3,4.5,1,0,1,0,32,10,0,1,1,0,0,3]])

print(PREDICTION)

"""**Final result come as 0 means Chirag is a Good customer according to ML model.**

# **For  MySQL  to perform the queries i need to save my clean data set into my computer for this lets process :- **
"""

from google.colab import files

"""**First I have to check my file which i want to download are there in my Colab environment Or not if not i need to save in colab and than download as:-**"""

import os

# List files in the current working directory
current_directory = os.getcwd()
file_list = os.listdir(current_directory)
print(file_list)

# Specify the file name and path within your Colab environment
file_path = '/content/data_miss_out2.csv'

# Save the DataFrame to a CSV file
data_miss_out2.to_csv(file_path, index=False)

from google.colab import files

# Download the CSV file to your local computer
files.download(file_path)

"""**Cleaned and complete data set i have downloaded from colab and than i have creat a new SCHEMA in MySql Workbench name 'Project'than i have done with all question which has been ask in MySql section As: **

use project;
select * from data_miss_out2;
# Q1. Group the customers based on their income type and find the average of their annual income.
select * from data_miss_out2 where Type_Income = 0;
select avg(Annual_income) from data_miss_out2 where Type_Income = 0;
select * from data_miss_out2 where Type_Income = 1;
select avg(Annual_income) from data_miss_out2 where Type_Income = 1;
select * from data_miss_out2 where Type_Income = 2;
select avg(Annual_income) from data_miss_out2 where Type_Income = 2;

# Q2.Find the female owners of cars and property.
select * from data_miss_out2 where GENDER = 0 and Car_Owner=0 and Propert_Owner = 0;
select count(GENDER) from data_miss_out2 where GENDER = 0 and Car_Owner=0 and Propert_Owner = 0;

# Q3 Find the male customers who are staying with their families.
select * from data_miss_out2 where GENDER =1 and Family_members >= 2;

# Q4. Please list the top five people having the highest income.
select * from data_miss_out2 order by Annual_income desc limit 5;

#Q5. How many married people are having bad credit?
select * from data_miss_out2 where Marital_status = 0 and label = 0;
select count(Marital_status) from data_miss_out2 where Marital_status = 0 and label = 1;

#Q6.What is the highest education level and what is the total count?
select count(EDUCATION) from data_miss_out2 where EDUCATION = 1 ;
select count(EDUCATION) from data_miss_out2 where EDUCATION = 2 ;
select count(EDUCATION) from data_miss_out2 where EDUCATION = 3 ;
# so the highest education level is 1 and total count of the same is 329.

#Q7. Between married males and females, who is having more bad credit?
select  count(GENDER) from data_miss_out2 where Marital_status =0 and label = 1 and GENDER = 1;
select  count(GENDER) from data_miss_out2 where Marital_status =0 and label = 1 and GENDER = 0;
# both are same in Numbers i.e 38 count.








 





